# ğŸ§  Tutorial: Sentence-BERT (sBERT) com Cosine Similarity Loss

Este repositÃ³rio demonstra **como treinar um modelo Sentence-BERT (sBERT)** para medir **similaridade entre sentenÃ§as** utilizando a funÃ§Ã£o de perda **ContrastiveLoss** da biblioteca [`sentence-transformers`](https://www.sbert.net/).

---

## ğŸ¯ Objetivos

- Entender como o **sBERT** gera embeddings semÃ¢nticos de sentenÃ§as.
- Compreender o papel de uma funÃ§Ã£o de perda (loss function) no treinamento de modelos de machine learning.
- Aprender o conceito e implementaÃ§Ã£o da **Cosine Similarity Loss**.
- Treinar um modelo supervisionado para prever o grau de **similaridade** entre pares de sentenÃ§as (0 = diferentes, 5 = muito semelhantes).
- Avaliar o modelo usando **Cosine Similarity** e **Mean Squared Error (MSE)**.

---

## ğŸ“š Conceitos Fundamentais

### ğŸ”¹ O que Ã© Sentence-BERT?

O **Sentence-BERT (sBERT)** Ã© uma variaÃ§Ã£o do modelo BERT ajustada para **comparar sentenÃ§as**.
Ele transforma frases em vetores numÃ©ricos **(embeddings)** que capturam o seu significado semÃ¢ntico.

Por exemplo:
- *â€œO gato dorme.â€* e *â€œO felino estÃ¡ descansando.â€* terÃ£o embeddings **prÃ³ximos**.
- *â€œO gato dorme.â€* e *â€œO carro Ã© vermelho.â€* terÃ£o embeddings **distantes**.

### ğŸ”¹ O que Ã© uma FunÃ§Ã£o de Perda?

A **funÃ§Ã£o de perda** (ou *loss function*) mede o quanto as previsÃµes do modelo estÃ£o **erradas** em relaÃ§Ã£o ao valor **esperado**.
Durante o treinamento, o modelo tenta **minimizar** essa perda, ajustando seus pesos internos a cada iteraÃ§Ã£o.

â¡ï¸ Assim, a funÃ§Ã£o de perda Ã© o mecanismo de aprendizado, ela informa ao modelo como melhorar.

### ğŸ”¹ O que Ã© a Cosine Similarity Loss?

A **Cosine Similarity Loss** ensina o modelo a produzir embeddings de sentenÃ§as de forma que o cosseno entre eles reflita sua **similaridade semÃ¢ntica**.

- Se duas sentenÃ§as tÃªm significados parecidos, o cosseno entre seus embeddings deve ser prÃ³ximo de 1.
- Se sÃ£o diferentes, o cosseno deve ser prÃ³ximo de -1.

#### ğŸ’¡ FÃ³rmula da Similaridade do Cosseno:
**cos_sim(A, B) = (A Â· B) / (â€–Aâ€– Ã— â€–Bâ€–)**

### ğŸ’¡ FÃ³rmula da Cosine Similarity Loss:
$$
L = 1 - \cos(A, B)
$$

Durante o treinamento, o modelo ajusta seus parÃ¢metros para **maximizar a similaridade** entre embeddings de sentenÃ§as que sÃ£o semanticamente prÃ³ximas e **minimizar a similaridade** entre as que nÃ£o sÃ£o.

---

## ğŸ”§ PrÃ©-requisitos
- Python: 3.11.5 (ou superior)

---

## âš™ï¸ InstalaÃ§Ã£o e configuraÃ§Ã£o

```bash
git clone https://github.com/leovianaf/sbert-contrastive-tutorial.git
cd sbert-contrastive-tutorial
```

Acessando o repositÃ³rio, vocÃª deve criar seu ambiente virtual, ativÃ¡-lo e instalar as dependÃªncias:
```bash
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

Com o ambiente virtual configurado, vocÃª deve utilizÃ¡-lo para executar os notebooks.

---

## ğŸš€ Treinamento
O notebook de treinamento pode ser acessado aqui: [`train_sbert_cosine.ipynb`](notebooks/train_sbert_cosine.ipynb)

---

## ğŸ“ˆ AvaliaÃ§Ã£o
O notebook de avaliaÃ§Ã£o do treinamento pode ser acessado aqui: [`evaluate_sbert.ipynb`](notebooks/evaluate_sbert.ipynb)

---

## ğŸ“Š Exemplo de resultado esperado
| sentence1                         | sentence2                               | similarity_score | predicted_similarity |
|-----------------------------------|------------------------------------------|------------------|----------------------|
| How do I learn ML?                | How to start learning machine learning?  | 4.8              | 4.6                  |
| What is the capital of Germany?   | How to become a software engineer?       | 1.3              | 1.2                  |

---

## ğŸ“œ LicenÃ§a

Este projeto estÃ¡ licenciado sob os termos da **MIT License**.
VocÃª Ã© livre para usar, modificar e distribuir este cÃ³digo, desde que mantenha o aviso de copyright.
